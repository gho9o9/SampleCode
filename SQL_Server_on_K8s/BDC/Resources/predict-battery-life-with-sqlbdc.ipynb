{
    "metadata": {
        "kernelspec": {
            "name": "pyspark3kernel",
            "display_name": "PySpark3"
        },
        "language_info": {
            "name": "pyspark3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "# Training and scoring within a SQL Big Data Cluster\r\n\r\nIn this notebook you will train a model, use it to score data that has been uploaded to HDFS, and save the scored result to an external table.\r\n\r\nWide World Importers has refrigerated trucks to deliver temperature-sensitive products. These are high-profit, and high-expense items. In the past, there have been failures in the cooling systems, and the primary culprit has been the deep-cycle batteries used in the system.\r\n\r\nWWI began replacing the batteries every three months as a preventative measure, but this has a high cost. Recently, the taxes on recycling batteries has increased dramatically. The CEO has asked the Data Science team if they can investigate creating a Predictive Maintenance system to more accurately tell the maintenance staff how long a battery will last, rather than relying on a flat 3 month cycle.\r\n\r\nThe trucks have sensors that transmit data to a file location. The trips are also logged. In this Jupyter Notebook, you'll create, train and store a Machine Learning model using SciKit-Learn, so that it can be deployed to multiple hosts.\r\n\r\nBegin by running the following cell. You can run any code cell by placing your cursor within its region and then selecting the play icon (a triangle within a circle) that appears on the left.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Import the standard modules we need\r\nimport pickle \r\nimport pandas as pd\r\nimport numpy as np\r\nimport datetime as dt\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.model_selection import train_test_split",
            "metadata": {},
            "outputs": [],
            "execution_count": 18
        },
        {
            "cell_type": "markdown",
            "source": "First, download the sensor data from the location where it is transmitted from the trucks, and load it into a Spark DataFrame.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "df = pd.read_csv('https://raw.githubusercontent.com/solliancenet/tech-immersion-data-ai/master/environment-setup/data/2/training-formatted.csv', header=0)\r\ndf.dropna()\r\nprint(df.shape)\r\nprint(list(df.columns))",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "(10000, 74)\n['Survival_In_Days', 'Province', 'Region', 'Trip_Length_Mean', 'Trip_Length_Sigma', 'Trips_Per_Day_Mean', 'Trips_Per_Day_Sigma', 'Battery_Rated_Cycles', 'Manufacture_Month', 'Manufacture_Year', 'Alternator_Efficiency', 'Car_Has_EcoStart', 'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first', 'Sensor_Reading_1', 'Sensor_Reading_2', 'Sensor_Reading_3', 'Sensor_Reading_4', 'Sensor_Reading_5', 'Sensor_Reading_6', 'Sensor_Reading_7', 'Sensor_Reading_8', 'Sensor_Reading_9', 'Sensor_Reading_10', 'Sensor_Reading_11', 'Sensor_Reading_12', 'Sensor_Reading_13', 'Sensor_Reading_14', 'Sensor_Reading_15', 'Sensor_Reading_16', 'Sensor_Reading_17', 'Sensor_Reading_18', 'Sensor_Reading_19', 'Sensor_Reading_20', 'Sensor_Reading_21', 'Sensor_Reading_22', 'Sensor_Reading_23', 'Sensor_Reading_24', 'Sensor_Reading_25', 'Sensor_Reading_26', 'Sensor_Reading_27', 'Sensor_Reading_28', 'Sensor_Reading_29', 'Sensor_Reading_30', 'Sensor_Reading_31', 'Sensor_Reading_32', 'Sensor_Reading_33', 'Sensor_Reading_34', 'Sensor_Reading_35', 'Sensor_Reading_36', 'Sensor_Reading_37', 'Sensor_Reading_38', 'Sensor_Reading_39', 'Sensor_Reading_40', 'Sensor_Reading_41', 'Sensor_Reading_42', 'Sensor_Reading_43', 'Sensor_Reading_44', 'Sensor_Reading_45', 'Sensor_Reading_46', 'Sensor_Reading_47', 'Sensor_Reading_48', 'Sensor_Reading_49', 'Sensor_Reading_50', 'Sensor_Reading_51', 'Sensor_Reading_52', 'Sensor_Reading_53', 'Sensor_Reading_54', 'Sensor_Reading_55', 'Sensor_Reading_56', 'Sensor_Reading_57', 'Sensor_Reading_58', 'Sensor_Reading_59', 'Sensor_Reading_60', 'Sensor_Reading_61']",
                    "output_type": "stream"
                }
            ],
            "execution_count": 19
        },
        {
            "cell_type": "markdown",
            "source": "After examining the data, the Data Science team selects certain columns that they believe are highly predictive of the battery life.\r\n\r\nNow, you will pick out the features and labels from the training data. Run the following cell.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Select the features used for predicting battery life\r\nx = df.iloc[:,1:74]\r\nx = x.iloc[:,np.r_[2:7, 9:73]]\r\nx = x.interpolate() \r\n\r\n# Select the labels only (the measured battery life) \r\ny = df.iloc[:,0].values.flatten()",
            "metadata": {},
            "outputs": [],
            "execution_count": 20
        },
        {
            "cell_type": "markdown",
            "source": "Run the following cell to view the features that will be used to train the model.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Examine the features selected \r\nprint(list(x.columns))",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "['Trip_Length_Mean', 'Trip_Length_Sigma', 'Trips_Per_Day_Mean', 'Trips_Per_Day_Sigma', 'Battery_Rated_Cycles', 'Alternator_Efficiency', 'Car_Has_EcoStart', 'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first', 'Sensor_Reading_1', 'Sensor_Reading_2', 'Sensor_Reading_3', 'Sensor_Reading_4', 'Sensor_Reading_5', 'Sensor_Reading_6', 'Sensor_Reading_7', 'Sensor_Reading_8', 'Sensor_Reading_9', 'Sensor_Reading_10', 'Sensor_Reading_11', 'Sensor_Reading_12', 'Sensor_Reading_13', 'Sensor_Reading_14', 'Sensor_Reading_15', 'Sensor_Reading_16', 'Sensor_Reading_17', 'Sensor_Reading_18', 'Sensor_Reading_19', 'Sensor_Reading_20', 'Sensor_Reading_21', 'Sensor_Reading_22', 'Sensor_Reading_23', 'Sensor_Reading_24', 'Sensor_Reading_25', 'Sensor_Reading_26', 'Sensor_Reading_27', 'Sensor_Reading_28', 'Sensor_Reading_29', 'Sensor_Reading_30', 'Sensor_Reading_31', 'Sensor_Reading_32', 'Sensor_Reading_33', 'Sensor_Reading_34', 'Sensor_Reading_35', 'Sensor_Reading_36', 'Sensor_Reading_37', 'Sensor_Reading_38', 'Sensor_Reading_39', 'Sensor_Reading_40', 'Sensor_Reading_41', 'Sensor_Reading_42', 'Sensor_Reading_43', 'Sensor_Reading_44', 'Sensor_Reading_45', 'Sensor_Reading_46', 'Sensor_Reading_47', 'Sensor_Reading_48', 'Sensor_Reading_49', 'Sensor_Reading_50', 'Sensor_Reading_51', 'Sensor_Reading_52', 'Sensor_Reading_53', 'Sensor_Reading_54', 'Sensor_Reading_55', 'Sensor_Reading_56', 'Sensor_Reading_57', 'Sensor_Reading_58', 'Sensor_Reading_59', 'Sensor_Reading_60', 'Sensor_Reading_61']",
                    "output_type": "stream"
                }
            ],
            "execution_count": 21
        },
        {
            "cell_type": "markdown",
            "source": "The lead Data Scientist believes that a standard Regression algorithm would do the best predictions.\r\n\r\nIn the following cell, you train a model using a GradientBoostingRegressor, providing it the features (X) and the label values (Y). Run the following cell.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Train a regression model \r\nfrom sklearn.ensemble import GradientBoostingRegressor \r\nmodel = GradientBoostingRegressor() \r\nmodel.fit(x,y)",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=100, n_iter_no_change=None, presort='auto',\n             random_state=None, subsample=1.0, tol=0.0001,\n             validation_fraction=0.1, verbose=0, warm_start=False)",
                    "output_type": "stream"
                }
            ],
            "execution_count": 22
        },
        {
            "cell_type": "markdown",
            "source": "Now try making a single prediction with the trained model. Run the following cell.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Try making a single prediction and observe the result\r\nmodel.predict(x.iloc[0:1])",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "array([1323.39791998])",
                    "output_type": "stream"
                }
            ],
            "execution_count": 23
        },
        {
            "cell_type": "markdown",
            "source": "With a trained model in hand, you are now ready to score battery life predictions against a new set of vehicle telemetry data. The output of the cell will be predicted battery life for each vehicle. Run the following cell.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# access the test data from HDFS by reading into a Spark DataFrame \r\ntest_data = pd.read_csv('https://raw.githubusercontent.com/solliancenet/tech-immersion-data-ai/master/environment-setup/data/2/fleet-formatted.csv', header=0)\r\ntest_data.dropna()\r\n\r\n# prepare the test data (dropping unused columns) \r\ntest_data = test_data.drop(columns=[\"Car_ID\", \"Battery_Age\"])\r\ntest_data = test_data.iloc[:,np.r_[2:7, 9:73]]\r\ntest_data.rename(columns={'Twelve_hourly_temperature_forecast_for_next_31_days _reversed': 'Twelve_hourly_temperature_history_for_last_31_days_before_death_l ast_recording_first'}, inplace=True) \r\n# make the battery life predictions for each of the vehicles in the test data \r\nbattery_life_predictions = model.predict(test_data) \r\n# examine the prediction \r\nbattery_life_predictions",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "array([1472.91111228, 1340.08897725, 1421.38601032, 1473.79033215,\n       1651.66584142, 1412.85617044, 1842.81351408, 1264.22762055,\n       1930.45602533, 1681.86345995])",
                    "output_type": "stream"
                }
            ],
            "execution_count": 24
        },
        {
            "cell_type": "markdown",
            "source": "Now you can package up the predictions along with the vehicle telemetry into a single DataFrame so that you can export it back out to HDFS as a CSV.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# prepare one data frame that includes predictions for each vehicle\r\nscored_data = test_data\r\nscored_data[\"Estimated_Battery_Life\"] = battery_life_predictions\r\n\r\ndf_scored = spark.createDataFrame(scored_data)\r\n\r\ndf_scored.coalesce(1).write.option(\"header\", \"true\").csv(\"/data/battery-life.csv\")",
            "metadata": {},
            "outputs": [],
            "execution_count": 28
        },
        {
            "cell_type": "markdown",
            "source": "The above command creates a folder called `battery-life.csv`, which contains one CSV file that you can create an external table from, which will enable you to query the predictions for each vehicle from SQL. Return to the lab instructions to learn how to create an external table you can use for querying this data using SQL.\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "## Optional - export and operationalize trained model\r\n\r\nOnce you are satisfied with the Model, you can save it out using the \"Pickle\" library for deployment to other systems.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "pickle_file = open('/tmp/pdm.pkl', 'wb')\r\npickle.dump(model, pickle_file)\r\nimport os\r\nprint(os.getcwd())\r\nos.listdir('///tmp')",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "/tmp/nm-local-dir/usercache/root/appcache/application_1561062262695_0003/container_1561062262695_0003_01_000001\n['Jetty_localhost_45411_datanode____.dlby3q', 'tmp1a_q3d2g', 'pdm.pkl', 'hsperfdata_root', 'Jetty_0_0_0_0_8042_node____19tj0x', 'nm-local-dir']",
                    "output_type": "stream"
                }
            ],
            "execution_count": 29
        },
        {
            "cell_type": "markdown",
            "source": "You could export this model and [run it at the edge or in SQL Server directly](https://azure.microsoft.com/en-us/services/sql-database-edge/). Here's an example of what that code could look like:\r\n\r\n```sql\r\nDECLARE @query_string nvarchar(max) -- Query Truck Data\r\nSET @query_string='\r\nSELECT ['Trip_Length_Mean', 'Trip_Length_Sigma', 'Trips_Per_Day_Mean', 'Trips_Per_Day_Sigma', 'Battery_Rated_Cycles', 'Alternator_Efficiency', 'Car_Has_EcoStart', 'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first', 'Sensor_Reading_1', 'Sensor_Reading_2', 'Sensor_Reading_3', 'Sensor_Reading_4', 'Sensor_Reading_5', 'Sensor_Reading_6', 'Sensor_Reading_7', 'Sensor_Reading_8', 'Sensor_Reading_9', 'Sensor_Reading_10', 'Sensor_Reading_11', 'Sensor_Reading_12', 'Sensor_Reading_13', 'Sensor_Reading_14', 'Sensor_Reading_15', 'Sensor_Reading_16', 'Sensor_Reading_17', 'Sensor_Reading_18', 'Sensor_Reading_19', 'Sensor_Reading_20', 'Sensor_Reading_21', 'Sensor_Reading_22', 'Sensor_Reading_23', 'Sensor_Reading_24', 'Sensor_Reading_25', 'Sensor_Reading_26', 'Sensor_Reading_27', 'Sensor_Reading_28', 'Sensor_Reading_29', 'Sensor_Reading_30', 'Sensor_Reading_31', 'Sensor_Reading_32', 'Sensor_Reading_33', 'Sensor_Reading_34', 'Sensor_Reading_35', 'Sensor_Reading_36', 'Sensor_Reading_37', 'Sensor_Reading_38', 'Sensor_Reading_39', 'Sensor_Reading_40', 'Sensor_Reading_41', 'Sensor_Reading_42', 'Sensor_Reading_43', 'Sensor_Reading_44', 'Sensor_Reading_45', 'Sensor_Reading_46', 'Sensor_Reading_47', 'Sensor_Reading_48', 'Sensor_Reading_49', 'Sensor_Reading_50', 'Sensor_Reading_51', 'Sensor_Reading_52', 'Sensor_Reading_53', 'Sensor_Reading_54', 'Sensor_Reading_55', 'Sensor_Reading_56', 'Sensor_Reading_57', 'Sensor_Reading_58', 'Sensor_Reading_59', 'Sensor_Reading_60', 'Sensor_Reading_61']\r\nFROM Truck_Sensor_Readings'\r\nEXEC [dbo].[PredictBattLife] 'pdm', @query_string;\r\n```",
            "metadata": {}
        }
    ]
}